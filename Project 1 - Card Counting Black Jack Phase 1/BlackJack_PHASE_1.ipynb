{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-mxtfOETQK1E"
      },
      "outputs": [],
      "source": [
        "# ========= Reinforcement Learning and Dynamic Optimization ======== #\n",
        "# -------------Card Counting Black Jack: Phase 1-------------------- #\n",
        "#\n",
        "# Michalis Lamprakis  2020030077\n",
        "# Dimitris Ilia       2020030200\n",
        "\n",
        "HIT  = 0\n",
        "STICK = 1\n",
        "A = [HIT, STICK]\n",
        "\n",
        "# BlackJack Environment.\n",
        "# Methods starts with _ naming convention for private methods (called only inside the environment).\n",
        "\n",
        "class BlackjackEnv:\n",
        "\n",
        "    # Constructor initializes the environment.\n",
        "    # Called with the creation of the environment.\n",
        "    def __init__(self):\n",
        "      self.reset()\n",
        "\n",
        "    # Deck creation.\n",
        "    # Each card is a tuple (rank, suit) e.g (4,hearts) , (A,spades)\n",
        "    def _init_deck(self):\n",
        "      suits = ['hearts', 'spades', 'diamonds', 'clubs']\n",
        "      ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
        "      self.deck = []\n",
        "      for suit in suits:\n",
        "          for rank in ranks:\n",
        "              self.deck.append((rank, suit))\n",
        "\n",
        "\n",
        "    # Card value function\n",
        "    # Calculate the value of each card.\n",
        "    def _card_value(self,card):\n",
        "        rank = card[0] # Take the rank of each card from the tuple\n",
        "        if rank in ['J', 'Q', 'K']:\n",
        "            return 10\n",
        "        elif rank == 'A':\n",
        "            return 11  # Initially count Ace as 11, change it later if needed\n",
        "        else:\n",
        "            return int(rank) # Else, each card counts as the number of the rank (it is a string so we convert it into int)\n",
        "\n",
        "    # Calculate hand value\n",
        "    def _hand_value(self,hand):\n",
        "        value = 0 # Total hand value\n",
        "        num_of_aces = 0 # Keeps track the num of aces\n",
        "\n",
        "        # For each card in hand sum the value and if there is an ace increase num_aces\n",
        "        for card in hand:\n",
        "            value += self._card_value(card)\n",
        "            if card[0] == 'A':\n",
        "                num_of_aces += 1\n",
        "        # While the total hand value is greater than 21 AND there is an ace in the game subtract 10 from the sum\n",
        "        # (this means we count the ace from 11 to 1) and deacrease the number of aces, repeat until sum is less than 21 or the num of aces is 0.\n",
        "        while (value > 21) and (num_of_aces > 0):\n",
        "            value -= 10  # convert an Ace from 11 to 1\n",
        "            num_of_aces -= 1\n",
        "\n",
        "        usable_ace = (num_of_aces > 0) # If num of aces is > 0 there is a usable ace so return it\n",
        "        return value, usable_ace\n",
        "\n",
        "\n",
        "    # Resets the game\n",
        "    def reset(self):\n",
        "        self._init_deck()\n",
        "        random.shuffle(self.deck) # Shuffle the deck\n",
        "\n",
        "        # Make user's and dealer hand.\n",
        "        # pop() removes the top card from the shuffled game_deck.\n",
        "        self.player = [self.deck.pop(), self.deck.pop()]\n",
        "        self.dealer = [self.deck.pop(), self.deck.pop()]\n",
        "\n",
        "        return self._get_obs(), {} # Returns the initial observation (state).\n",
        "\n",
        "\n",
        "    # Obs stands for observation\n",
        "    # Returns the state of the game as a tuple of {players hand value, dealer's first visible card, wheather player have a usable ace or not, truncated, info}\n",
        "    # We use the standard open AI gym format even thow we dont use all the elements from the tuple\n",
        "    def _get_obs(self):\n",
        "        value, usable_ace = self._hand_value(self.player)\n",
        "        return (value,\n",
        "                self._card_value(self.dealer[0]),   # 2..11\n",
        "                int(usable_ace))                    # 0 or 1\n",
        "\n",
        "    # Handles the game logic for each action.\n",
        "    # returns a tuple {new state obervation, reward (1,0,-1), is the game terminating (boolean) }\n",
        "    def step(self, action):\n",
        "        # Player hits\n",
        "        if action == HIT:\n",
        "            self.player.append(self.deck.pop())\n",
        "            value, _ = self._hand_value(self.player)\n",
        "\n",
        "            # If value is > 21 player loses and game ends else game continues.\n",
        "            if value > 21:\n",
        "                return self._get_obs(), -1, True, False, {}\n",
        "            else:\n",
        "                return self._get_obs(), 0,  False, False, {}\n",
        "\n",
        "        # Else player sticks → dealer’s turn\n",
        "        # If dealer's hand value is 16 or less the dealer MUST draw another card else the dealer MUST stay.\n",
        "        # Here we return terminal reward that checks the new conditions (returns teh same tuple)\n",
        "        dealer_value, _ = self._hand_value(self.dealer)\n",
        "        while dealer_value < 17:\n",
        "            self.dealer.append(self.deck.pop())\n",
        "            dealer_value, _ = self._hand_value(self.dealer)\n",
        "        return self._terminal_reward()\n",
        "\n",
        "    # Calculates final result.\n",
        "    # returns a tuple sa before {new state obervation, reward (1,0,-1), is the game terminating (boolean), truncated, info }\n",
        "    def _terminal_reward(self):\n",
        "        player_total, _ = self._hand_value(self.player)\n",
        "        dealer_total, _ = self._hand_value(self.dealer)\n",
        "        if dealer_total > 21 or player_total > dealer_total:  # Player wins\n",
        "            reward = 1\n",
        "        elif player_total < dealer_total:                     # Dealer's win\n",
        "            reward = -1\n",
        "        else:\n",
        "            reward = 0                                        # It's a draw\n",
        "        return self._get_obs(), reward, True, False, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement just a random policy\n",
        "def evaluate_random_policy(n_games=100_000):\n",
        "    env = BlackjackEnv()\n",
        "    wins = draws = losses = 0\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = random.choice([HIT, STICK])\n",
        "            s, r, done, _, _ = env.step(a)\n",
        "        if r == 1:\n",
        "            wins += 1\n",
        "        elif r == 0:\n",
        "            draws += 1\n",
        "        else:\n",
        "            losses += 1\n",
        "    total = wins + draws + losses\n",
        "    return wins / total, draws / total, losses / total\n"
      ],
      "metadata": {
        "id": "uKy5kS4PnNh4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a simple threshold policy, hits if the player's total is below a fixed threshold (e.g. 17)\n",
        "\n",
        "def threshold_policy(player_total, threshold=17):\n",
        "    return HIT if player_total < threshold else STICK\n",
        "\n",
        "\n",
        "def evaluate_threshold_policy(threshold=17, n_games=100_000):\n",
        "    env = BlackjackEnv()\n",
        "    wins = draws = losses = 0\n",
        "\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            player_total = s[0]\n",
        "            action = threshold_policy(player_total, threshold)\n",
        "            s, r, done, _, _ = env.step(action)\n",
        "\n",
        "        if r == 1:\n",
        "            wins += 1\n",
        "        elif r == 0:\n",
        "            draws += 1\n",
        "        else:\n",
        "            losses += 1\n",
        "\n",
        "    total = wins + draws + losses\n",
        "    return wins / total, draws / total, losses / total\n"
      ],
      "metadata": {
        "id": "Hz_rFZpepU1H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uHurLmUkUbWD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import tqdm                             # This is for the progress bar\n",
        "\n",
        "# Q Table creation\n",
        "# defaultdict: Automatic state creation when the agent encounters a new state. The state tuple becomes a dictionary key and the default Q values are [0.0, 0.0].\n",
        "# We expect 360x2 table because:\n",
        "# Player's hand total: 4-21 (18 possible values) (e.g A and 2 is not 3 but 13 that's why)\n",
        "# Dealer's visible card: 2-11 (10 possible values)\n",
        "# Usable ace: Yes/No (2 possible values)\n",
        "# 18 × 10 × 2 = 360 possible state combinations\n",
        "# And 2 possible actions HIT or STICK\n",
        "Q = defaultdict(lambda: np.zeros(2))\n",
        "\n",
        "\n",
        "alpha = 0.1                                     # Learning rate (how aggressively we update values)\n",
        "gamma = 1.0                                     # Discount factor (1.0 = no discount for future rewards)\n",
        "e_start = 1.0                                   # Initial exploration rate\n",
        "e_end = 0.05                                    # Minimum exploration rate\n",
        "e_decay = 1e5                                   # Rate of exploration deca\n",
        "episodes = 500_000                              # Number of episodes\n",
        "\n",
        "\n",
        "\n",
        "# ε-Greedy Policy\n",
        "# Balances exploration (random actions) with exploitation (best known actions)\n",
        "# Starts with 100% exploration (epsilon=1.0), decays over time\n",
        "def policy(state, eps):\n",
        "    if random.random() < eps:\n",
        "        return random.choice(A)\n",
        "    return int(np.argmax(Q[state]))\n",
        "\n",
        "# Training part\n",
        "def train():\n",
        "  env = BlackjackEnv()                            # Creates a new environment\n",
        "  eps = e_start\n",
        "  for ep in tqdm.tqdm(range(episodes)):\n",
        "      # Reset environment in every loop\n",
        "      state, _ = env.reset()\n",
        "      terminated = False\n",
        "\n",
        "      # Continue playing while terminaded is not false.\n",
        "      while not terminated:\n",
        "          # Action is HIT or STICK\n",
        "          action = policy(state, eps)\n",
        "          next_state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "\n",
        "          if terminated:\n",
        "              td_target = reward                   # no bootstrap\n",
        "          else:\n",
        "              best_next  = np.max(Q[next_state])\n",
        "              td_target  = reward + gamma * best_next\n",
        "\n",
        "          td_error  = td_target - Q[state][action]\n",
        "          Q[state][action] += alpha * td_error\n",
        "          state = next_state\n",
        "\n",
        "      # Simple linear decay\n",
        "      # Linearly reduces exploration rate from 1.0 to 0.05 over episodes\n",
        "      eps = max(e_end, eps - (e_start-e_end) / e_decay)\n",
        "\n",
        "# Plays 100,000 games to get statistically performance metrics\n",
        "# Play with the learned policy\n",
        "def evaluate(agent_Q, n_games=100_000):\n",
        "    env = BlackjackEnv()\n",
        "    wins = draws = losses = 0\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = int(np.argmax(agent_Q[s]))  # Returns the index of the highest Q-value in the array.(0=HIT, 1=STICK)\n",
        "            s, r, done, _, _ = env.step(a)\n",
        "        if r == 1:\n",
        "            wins  += 1\n",
        "        elif r == 0:\n",
        "            draws += 1\n",
        "        else:\n",
        "            losses += 1\n",
        "    total = wins + draws + losses\n",
        "    return wins/total, draws/total, losses/total\n",
        "\n",
        "def play_blackjack_manually():\n",
        "    env = BlackjackEnv()\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Print player's starting card and show only the one of the dealer's. (also calculate the value)\n",
        "    #****also minor changes to prints getting from enviroments so tuples of cards to be shown here\n",
        "    print(f\"\\nYour hand: {env.player} Your starting hand value: {state[0]}, dealer shows: {env.dealer[0]}\")\n",
        "\n",
        "    while not done:\n",
        "        action = input(\"Do you want to (h)it or (s)tick? \").lower()\n",
        "        if action not in ['h', 's']:\n",
        "            print(\"Invalid input. Please choose 'h' or 's'.\")\n",
        "            continue\n",
        "        # If action = HIT call step with action_code = 0 means hit. The same with STICK\n",
        "        if action == 'h':\n",
        "            action_code = 0\n",
        "        else:\n",
        "            action_code = 1\n",
        "        state, reward, done, _, _ = env.step(action_code)\n",
        "        # Print the results\n",
        "\n",
        "        #*** my changes here so i can get more prints to se hand values as would happen if you played blackjack in a casino\n",
        "        if not done and action_code == 0:\n",
        "            print(f\"New state: Your hand: {env.player}, hand value = {state[0]}, dealer shows: {env.dealer[0]}\")\n",
        "        elif not done and action_code == 1:\n",
        "            print(f\"New state: Your hand: {env.player}, hand value = {state[0]}, dealer shows: {env.dealer}\")\n",
        "        else:\n",
        "            if reward == 1:\n",
        "                print(f\"Final state: Your hand: {env.player}, hand value = {state[0]}, dealer shows: {env.dealer}\")\n",
        "                print(\"You win!\")\n",
        "\n",
        "            elif reward == -1:\n",
        "                print(f\"Final state: Your hand: {env.player}, hand value = {state[0]}, dealer shows: {env.dealer}\")\n",
        "                print(\"Dealer wins!\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Final state: Your hand: {env.player}, hand value = {state[0]}, dealer shows: {env.dealer}\")\n",
        "                print(\"It's a draw!\")\n",
        "\n",
        "\n",
        "# Implementation of best proven strategy for STICK/HIT Poker game (no double etc).\n",
        "# The strategy is widely used in casinos (ref: https://www.blackjackapprenticeship.com/blackjack-strategy-charts/)\n",
        "def best_strategy_action(player_total, dealer_card, usable_ace):\n",
        "  #SOFT Hands\n",
        "  if usable_ace:\n",
        "    if player_total >= 19:\n",
        "        return STICK\n",
        "    elif player_total == 18:\n",
        "        if dealer_card in [2, 7, 8]:\n",
        "            return STICK\n",
        "        else:\n",
        "            return HIT\n",
        "    else:\n",
        "        return HIT\n",
        "  #HARD Hands\n",
        "  else:\n",
        "    if player_total >= 17:\n",
        "        return STICK\n",
        "    elif 13 <= player_total <= 16:\n",
        "        if dealer_card <= 6:\n",
        "            return STICK\n",
        "        else:\n",
        "            return HIT\n",
        "    elif player_total == 12:\n",
        "        if 4 <= dealer_card <= 6:\n",
        "            return STICK\n",
        "        else:\n",
        "            return HIT\n",
        "    else:\n",
        "        return HIT\n",
        "\n",
        "# Function that compares our learned policy to actual basic strategy\n",
        "def compare_to_best_strategy():\n",
        "    mismatches = 0\n",
        "    total = 0\n",
        "    # Only tests hands where decision-making matters.\n",
        "    # Hands below 12 always require a hit, so they’re usually excluded.\n",
        "    for player_total in range(12, 22):\n",
        "        for dealer_card in range(2, 12):\n",
        "            for usable_ace in [0, 1]:\n",
        "                state = (player_total, dealer_card, usable_ace)\n",
        "                agent_action = int(np.argmax(Q[state]))\n",
        "                strategy_action = best_strategy_action(player_total, dealer_card, usable_ace)\n",
        "                if agent_action != strategy_action:\n",
        "                    print(f\"Mismatch: State {state}, Agent: {agent_action}, Strategy: {strategy_action}\")\n",
        "                    mismatches += 1\n",
        "                total += 1\n",
        "    print(f\"\\nTotal mismatches: {mismatches}/{total} ({(mismatches/total)*100:.2f}% off-strategy)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# w, d, l = evaluate(Q)\n",
        "# print(f\"Win {w:.2%}   Draw {d:.2%}   Lose {l:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pxYVhX1kK7sD"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "#  Hi-Lo single-deck environment (inherits almost everything)\n",
        "# ---------------------------------------------------------------------\n",
        "class BlackjackHiLoEnv(BlackjackEnv):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.running_count = 0   # shared across hands until shoe is shuffled\n",
        "        self._init_deck()\n",
        "        random.shuffle(self.deck)\n",
        "        super().__init__()       # does a reset()\n",
        "\n",
        "    # ---------- Hi-Lo helpers ----------------------------------------\n",
        "    #return the hi-lo value of a card\n",
        "    @staticmethod\n",
        "    def _hilo_value(card):\n",
        "        rank = card[0]\n",
        "        if rank in ['2', '3', '4', '5', '6']:\n",
        "            return +1\n",
        "        elif rank in ['7', '8', '9']:\n",
        "            return 0\n",
        "        else:                                   # 10, J, Q, K, A\n",
        "            return -1\n",
        "\n",
        "    #in which state are we?\n",
        "    def _hiLoState(self):\n",
        "        if   self.running_count >  3: return 2  # High\n",
        "        elif self.running_count < -3: return 0  # Low\n",
        "        return 1                                # Neutral\n",
        "\n",
        "    # ---------- deck management --------------------------------------\n",
        "    def _prepare_deck(self):\n",
        "        \"\"\"Shuffle when ≤10 cards remain and wipe the running count.\"\"\"\n",
        "        if len(self.deck) <= 10:\n",
        "            self._init_deck()\n",
        "            random.shuffle(self.deck)\n",
        "            self.running_count = 0\n",
        "\n",
        "    def _draw(self):\n",
        "        \"\"\"Pop a card and update the Hi-Lo running count.\"\"\"\n",
        "        card = self.deck.pop()\n",
        "        self.running_count += self._hilo_value(card)\n",
        "        return card\n",
        "\n",
        "    # ---------- overrides --------------------------------------------\n",
        "    #difference from *basic* reset is it only resets if cards are lower than 10\n",
        "    def reset(self):\n",
        "        self._prepare_deck()\n",
        "        self.player = [self._draw(), self._draw()]\n",
        "        self.dealer = [self._draw(), self._draw()]\n",
        "        return self._get_obs(), {}\n",
        "    #same as *basic* get observation space but with the added hilo state, hilo state from dealer is only the face upcard\n",
        "    def _get_obs(self):\n",
        "        value, usable_ace = self._hand_value(self.player)\n",
        "        upcard = self._card_value(self.dealer[0])     # 2-11\n",
        "        return (value, upcard, int(usable_ace), self._hiLoState())\n",
        "\n",
        "    #the step is the same as *basic*\n",
        "    def step(self, action):\n",
        "        if action == HIT:                     # player hits\n",
        "            self.player.append(self._draw())\n",
        "            value, _ = self._hand_value(self.player)\n",
        "            if value > 21: #bigger than 21 we lose\n",
        "                return self._get_obs(), -1, True, False, {}\n",
        "            return self._get_obs(), 0, False, False, {} #continue\n",
        "\n",
        "        # player sticks → dealer plays\n",
        "        dealer_value, _ = self._hand_value(self.dealer)\n",
        "        while dealer_value < 17:  #hit until >17\n",
        "            self.dealer.append(self._draw())\n",
        "            dealer_value, _ = self._hand_value(self.dealer)\n",
        "        return self._terminal_reward()  #return the reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MEqZehxLdSqe"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "#  Card-counting learner\n",
        "# ---------------------------------------------------------------------\n",
        "Q_count = defaultdict(lambda: np.zeros(2))\n",
        "\n",
        "#what choice we take\n",
        "def policy_count(state, eps):\n",
        "    if random.random() < eps: #if random < ε then we take random choice\n",
        "        return random.choice(A)\n",
        "    return int(np.argmax(Q_count[state])) #else take best course action\n",
        "\n",
        "#start training\n",
        "def train_count(episodes):\n",
        "    env = BlackjackHiLoEnv()#hilo enviroment\n",
        "    eps = e_start\n",
        "    for _ in tqdm.tqdm(range(episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a  = policy_count(state, eps)\n",
        "            nxt, r, done, _, _ = env.step(a)\n",
        "\n",
        "            # TD target\n",
        "            td_target = r if done else r + gamma * np.max(Q_count[nxt])\n",
        "            # TD update\n",
        "            Q_count[state][a] += alpha * (td_target - Q_count[state][a])\n",
        "            state = nxt\n",
        "\n",
        "        eps = max(e_end, eps - (e_start - e_end) / e_decay)\n",
        "\n",
        "#evaluate\n",
        "def evaluate_count(n_games=1000_000):\n",
        "    env = BlackjackHiLoEnv()\n",
        "    w = d = l = 0\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset()\n",
        "        #print(env.deck)\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = int(np.argmax(Q_count[s]))\n",
        "            s, r, done, _, _ = env.step(a)\n",
        "        if   r == 1: w += 1\n",
        "        elif r == 0: d += 1\n",
        "        else:        l += 1\n",
        "    tot = w + d + l\n",
        "    return w/tot, d/tot, l/tot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDgZbEv_dwZC"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "#  Pretty-print helper for either Q or Q_count(hilo)\n",
        "# ---------------------------------------------------------------------\n",
        "def print_q_table(table, count_version=False, max_rows=None):\n",
        "    \"\"\"\n",
        "    table          – defaultdict holding numpy[2] (Q or Q_count)\n",
        "    count_version  – True if the key is (player, upcard, ace, bucket)\n",
        "    max_rows       – optional int to limit lines displayed\n",
        "    \"\"\"\n",
        "    def bucket_name(b):\n",
        "        return {0: \"Low \", 1: \"Neut\", 2: \"High\"}[b]\n",
        "\n",
        "    rows = 0\n",
        "    for key in sorted(table.keys()):\n",
        "        if count_version:\n",
        "            p, d, ace, bucket = key\n",
        "            bucket_str = bucket_name(bucket)\n",
        "            state_str  = f\"(P={p:>2}, D={d:>2}, A={ace}, C={bucket_str})\"\n",
        "        else:                                    # basic 3-tuple version\n",
        "            p, d, ace = key\n",
        "            state_str = f\"(P={p:>2}, D={d:>2}, A={ace})\"\n",
        "\n",
        "        q_hit, q_stick = table[key]\n",
        "        best = \"HIT\" if q_hit > q_stick else \"STICK\"\n",
        "\n",
        "        # right-align the whole state column to 26 chars for neatness\n",
        "        print(f\"S={state_str:>26} | \"\n",
        "              f\"Q_hit={q_hit:+.3f}  Q_stick={q_stick:+.3f}  -> {best}\")\n",
        "\n",
        "        rows += 1\n",
        "        if max_rows and rows >= max_rows:\n",
        "            print(\"… (truncated) …\")\n",
        "            break\n",
        "\n",
        "\n",
        "# MENU\n",
        "def main_menu():\n",
        "    while True:\n",
        "        print(\"\\n=== BLACKJACK MENU ===\")\n",
        "        print(\"1. Play against the dealer (manual)\")\n",
        "        print(\"2. Evaluate random policy\")\n",
        "        print(\"3. Evaluate threshold policy\")\n",
        "        print(\"4. Train *basic* Q-agent\")\n",
        "        print(\"5. Evaluate *basic* agent\")\n",
        "        print(\"6. Compare to basic-strategy chart\")\n",
        "        print(\"7. Train *Hi-Lo* counting agent\")\n",
        "        print(\"8. Evaluate *Hi-Lo* agent\")\n",
        "        print(\"9. Exit\")\n",
        "\n",
        "\n",
        "        choice = input(\"Select an option (1-9): \")\n",
        "        if choice == '1':\n",
        "            play_blackjack_manually()\n",
        "        elif choice == '2':\n",
        "            w, d, l = evaluate_random_policy()\n",
        "            print(f\"Win {w:.2%}  Draw {d:.2%}  Lose {l:.2%}\")\n",
        "        elif choice == '3':\n",
        "            w, d, l = evaluate_threshold_policy()\n",
        "            print(f\"Win {w:.2%}  Draw {d:.2%}  Lose {l:.2%}\")\n",
        "        elif choice == '4':\n",
        "            train()\n",
        "        elif choice == '5':\n",
        "            w, d, l = evaluate(Q)\n",
        "            print(f\"Win {w:.2%}  Draw {d:.2%}  Lose {l:.2%}\")\n",
        "        elif choice == '6':\n",
        "            compare_to_best_strategy()\n",
        "        elif choice == '7':\n",
        "            train_count(episodes=1_500_000)\n",
        "            # Print the card-counting table (all rows\n",
        "        elif choice == '8':\n",
        "            w, d, l = evaluate_count(n_games=1_000_000)\n",
        "            print(f\"Win {w:.2%}  Draw {d:.2%}  Lose {l:.2%}\")\n",
        "        # elif choice == '7':\n",
        "        #     print_q_table(Q_count, count_version=False)\n",
        "        # elif choice == '8':\n",
        "        #     print_q_table(Q_count, count_version=True)\n",
        "        elif choice == '9':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_menu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtzExUlJYt_d"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}